{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import page\n",
    "import requests as rq\n",
    "base_url = 'https://minimisj2.bge-hautsdefrance.fr/contactsLDAP_photo_modal.php'\n",
    "#\n",
    "id= '01717140066145957'\n",
    "page = rq.get(f\"{base_url}?id={id}\", auth=('asibille', 'Electro22@AppliRH'))\n",
    "list = page.content\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beautifulsoup\n",
    "from bs4 import BeautifulSoup\n",
    "list_bs = BeautifulSoup(list, 'html.parser')\n",
    "list_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract names list\n",
    "list_name = list_bs.find_all('h1')\n",
    "list_name = [name.get_text() for name in list_name]\n",
    "list_name\n",
    "len(list_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract positions list\n",
    "list_position = list_bs.find_all('h2')\n",
    "list_position = [position.get_text() for position in list_position]\n",
    "list_position\n",
    "len(list_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract agencies list\n",
    "list_agency = list_bs.find_all('h3')\n",
    "#avoid doubled names: Use list comprehension along with enumeration to filter out duplicate agency names\n",
    "list_agency = [agency.get_text() for key, agency in enumerate(list_agency) if key % 2 == 0]\n",
    "list_agency = [agency.replace(\"'\", '\"') for agency in list_agency]\n",
    "list_agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt to zip list\n",
    "try:\n",
    "    list(zip(list_name, list_position, list_agency))\n",
    "\n",
    "except Exception as ex:\n",
    "    print(f\"An unexpected exception occurred: {ex}\")\n",
    "    # Handle other exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Dataframe based on a previous created dictionary\n",
    "global_list = {'name' : list_name,\n",
    "               'position' : list_position,\n",
    "               'agency' : list_agency}\n",
    "global_list = pd.DataFrame(global_list)\n",
    "global_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter rows where two agencies are mentioned\n",
    "mask = global_list['agency'].str.contains('\\+')\n",
    "global_list_rowsPlus = global_list.loc[mask]\n",
    "\n",
    "global_list_unmasked = global_list[~mask]\n",
    "\n",
    "len(global_list_rowsPlus)\n",
    "\n",
    "new_rows = [] #create empty list for append new rows copying\n",
    "rows_drop = [] #create empty list for append old rows to erase\n",
    "\n",
    "for indexr, row in global_list_rowsPlus.iterrows():\n",
    "    agencySplits = row['agency'].split('+')\n",
    "    if len(agencySplits) > 1:\n",
    "    #if len(grlps)\n",
    "        for agency in agencySplits:\n",
    "            new_row = row.copy()  # Make a copy of the original row\n",
    "            new_row['agency'] = agency.strip()  # Update the agency\n",
    "            new_rows.append(new_row) # Append the new row to the list\n",
    "        rows_drop.append(indexr) # Append the index of rows to drop\n",
    "            # global_list_rowsPlus.drop(index=indexr)  \n",
    "    else:\n",
    "        new_rows.append(row)\n",
    "        # Duplicate rows by concatenating DataFrame with itself\n",
    "\n",
    "# Create a new DataFrame with the modified rows based on global_list_rowsPlus\n",
    "global_list_rowsPlus_modified = pd.DataFrame(new_rows, columns=global_list_rowsPlus.columns)\n",
    "\n",
    "# Drop the rows to be dropped from the original DataFrame\n",
    "global_list_rowsPlus.drop(index=rows_drop, inplace=True)\n",
    "\n",
    "# concatenate lists updated\n",
    "global_list_rowsPlus = pd.concat([global_list_rowsPlus, global_list_rowsPlus_modified])\n",
    "\n",
    "#by concatening we add initial rows of global list with '+' \n",
    "global_list_duplicated = pd.concat([global_list_unmasked, global_list_rowsPlus], ignore_index=True)\n",
    "\n",
    "#sort list\n",
    "global_list_duplicated_sorted = global_list_duplicated.sort_values(['name', 'agency'])\n",
    "\n",
    "global_list_duplicated_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "\n",
    "!python -m pip install unidecode -U\n",
    "import unidecode\n",
    "\n",
    "\n",
    "\n",
    "# Define lambdas for transformations\n",
    "lower_index = lambda x: x.lower() if isinstance(x, str) else x\n",
    "erase_indent = lambda x: x.replace('-', ' ') if isinstance(x, str) else x\n",
    "strip_index = lambda x: x.strip() if isinstance(x, str) else x\n",
    "accents_strip = lambda x: unidecode.unidecode(x) if isinstance(x, str) else x   # Apply unidecode only to string values\n",
    "erase_apostrophe = lambda x: x.replace(\"'\", \" \") if isinstance(x, str) else x\n",
    "\n",
    "# Identify columns with dtype 'object' (string columns)\n",
    "str_columns_list = [str_column for str_column in global_list_duplicated_sorted.columns if global_list_duplicated_sorted[str_column].dtype == 'object']\n",
    "\n",
    "# Apply transformations to selected string columns\n",
    "for column in str_columns_list:\n",
    "    global_list_duplicated_sorted[column] = global_list_duplicated_sorted[column].apply(lower_index).apply(erase_indent).apply(strip_index).apply(accents_strip).apply(erase_apostrophe)\n",
    "\n",
    "# Output the transformed DataFrame\n",
    "global_list_duplicated_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_list_duplicated_sorted.to_csv('people_global_list_05_2024.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where two agencies are mentioned\n",
    "mask = global_list['agency'].str.contains('\\+')\n",
    "global_list_rowsPlus = global_list.loc[mask]\n",
    "\n",
    "# Duplicate rows by concatenating DataFrame with itself\n",
    "global_list_duplicated = pd.concat([global_list, global_list_rowsPlus], ignore_index=True)\n",
    "\n",
    "# Sort the duplicated DataFrame by 'agency'\n",
    "global_list_duplicated_sorted = global_list_duplicated.sort_values(by=['name', 'agency'])\n",
    "\n",
    "# Erase leading and trailing spaces in column 'agency'\n",
    "global_list_duplicated_sorted['agency'] = global_list_duplicated_sorted['agency'].apply(lambda x: x.strip())\n",
    "\n",
    "# Function to split and keep the left part for the first duplicated line\n",
    "def splitLeft_replace(x):\n",
    "    split_parts = x.split('+')\n",
    "    il = len(split_parts)\n",
    "    if il > 1:\n",
    "        return split_parts[0]  # Keep the first part\n",
    "    else:\n",
    "        return x  # Return the original value if no '+' sign found\n",
    "\n",
    "# Function to split and keep the right part for the second duplicated line\n",
    "def splitRight_replace(x):\n",
    "    split_parts = x.rsplit('+')\n",
    "    il = len(split_parts)\n",
    "    if il == 1:\n",
    "        return split_parts[1 - il]  # Keep the last part\n",
    "    elif il > 1:\n",
    "        return split_parts[il - 1]\n",
    "    else:\n",
    "        return x  # Return the original value if no '+' sign found\n",
    "\n",
    "def defineSplitSide(x):\n",
    "# Iterate over the DataFrame to handle duplicated rows\n",
    "    for i in range(len(x) - 1):  # Exclude the last row to prevent index out of range\n",
    "        current_agency = x.iloc[i, -1]\n",
    "        next_agency = x.iloc[i + 1, -1]\n",
    "        if current_agency == next_agency:\n",
    "            #if i % 2 == 0:  # Even index (first duplicated line)\n",
    "                # Apply splitLeft_replace function to clean the 'agency' column\n",
    "            x.iloc[i, -1] = splitLeft_replace(current_agency)\n",
    "            \n",
    "            #else:  # Odd index (second duplicated line)\n",
    "                # Apply splitRight_replace function to clean the 'agency' column\n",
    "            x.iloc[i + 1, -1] = splitRight_replace(next_agency)\n",
    "        else:\n",
    "            continue\n",
    "    return global_list_duplicated_sorted    \n",
    "\n",
    "\n",
    "defineSplitSide(global_list_duplicated_sorted)\n",
    "\n",
    "masktest = global_list_duplicated_sorted['name'] == 'Arnaud SIBILLE' \n",
    "global_list_duplicated_sorted.loc[masktest]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where two agencies are mentioned\n",
    "mask = global_list['agency'].str.contains('\\+')\n",
    "global_list_rowsPlus = global_list[mask]\n",
    "\n",
    "# Duplicate rows by concatenating DataFrame with itself\n",
    "global_list_duplicated = pd.concat([global_list, global_list_rowsPlus], ignore_index=True)\n",
    "\n",
    "# Sort the duplicated DataFrame by 'agency'\n",
    "global_list_duplicated_sorted = global_list_duplicated.sort_values(by=['name', 'agency'])\n",
    "\n",
    "# Function to split and keep the left part for the first duplicated line\n",
    "def splitLeft_replace(x):\n",
    "    split_parts = x.replace('+', ' ').split(' ')\n",
    "    if len(split_parts) > 1:\n",
    "        return split_parts[0]  # Keep the first part\n",
    "    else :\n",
    "        return x  # Return the original value if no '+' sign found\n",
    "\n",
    "# Function to split and keep the right part for the second duplicated line\n",
    "def splitRight_replace(x):\n",
    "    split_parts = x.replace('+', ' ').split(' ')\n",
    "    il = len(split_parts)\n",
    "    if il > 1:\n",
    "        return split_parts[il - 1]  # Keep the last part\n",
    "    else:\n",
    "        return x  # Return the original value if no '+' sign found\n",
    "\n",
    "\n",
    "# Apply splitLeft_replace and splitRight_replace functions to clean the 'agency' column\n",
    "## mistake : by first applying to all the column def leftreplace => the agency column contains only one word in every rows, so impossible after to apply def rightReplace \n",
    "global_list_duplicated_sorted['agency'] = global_list_duplicated_sorted['agency'].apply(splitLeft_replace)\n",
    "\n",
    "# Filter the second duplicated row and apply splitRight_replace to clean the 'agency' column\n",
    "second_duplicated_mask = global_list_duplicated_sorted[['name', 'agency']].duplicated()\n",
    "global_list_duplicated_sorted.loc[second_duplicated_mask, 'agency'] = global_list_duplicated_sorted.loc[second_duplicated_mask, 'agency'].apply(splitRight_replace)\n",
    "\n",
    "global_list_duplicated_sorted\n",
    "\n",
    "masktest = global_list_duplicated_sorted['name'] == 'Arnaud SIBILLE' \n",
    "global_list_duplicated_sorted.loc[masktest]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where two agencies are mentioned\n",
    "mask = global_list['agency'].str.contains('\\+')\n",
    "global_list_rowsPlus = global_list.loc[mask]\n",
    "\n",
    "# Duplicate rows by concatenating DataFrame with itself\n",
    "global_list_duplicated = pd.concat([global_list, global_list_rowsPlus], ignore_index=True)\n",
    "\n",
    "# Sort the duplicated DataFrame by 'agency'\n",
    "global_list_duplicated_sorted = global_list_duplicated.sort_values(by=['name', 'agency'])\n",
    "\n",
    "# Erase leading and trailing spaces in column 'agency'\n",
    "global_list_duplicated_sorted['agency'] = global_list_duplicated_sorted['agency'].apply(lambda x: x.strip())\n",
    "\n",
    "# Function to split and keep the left part for the first duplicated line\n",
    "def splitLeft_replace(x):\n",
    "    split_parts = x.split('+')\n",
    "    il = len(split_parts)\n",
    "    if il > 1:\n",
    "        return split_parts[0]  # Keep the first part\n",
    "    else:\n",
    "        return x  # Return the original value if no '+' sign found\n",
    "\n",
    "# Function to split and keep the right part for the second duplicated line\n",
    "def splitRight_replace(x):\n",
    "    split_parts = x.rsplit('+')\n",
    "    il = len(split_parts)\n",
    "    if il == 1:\n",
    "        return split_parts[1 - il]  # Keep the last part\n",
    "    elif il > 1:\n",
    "        return split_parts[il - 1]\n",
    "    else:\n",
    "        return x  # Return the original value if no '+' sign found\n",
    "\n",
    "def defineSplitSide(x):\n",
    "# Iterate over the DataFrame to handle duplicated rows\n",
    "    for i in range(len(x) - 1):  # Exclude the last row to prevent index out of range\n",
    "        current_agency = x.iloc[i, -1]\n",
    "        next_agency = x.iloc[i + 1, -1]\n",
    "        if current_agency == next_agency:\n",
    "            #if i % 2 == 0:  # Even index (first duplicated line)\n",
    "                # Apply splitLeft_replace function to clean the 'agency' column\n",
    "            x.iloc[i, -1] = splitLeft_replace(current_agency)\n",
    "            \n",
    "            #else:  # Odd index (second duplicated line)\n",
    "                # Apply splitRight_replace function to clean the 'agency' column\n",
    "            x.iloc[i + 1, -1] = splitRight_replace(next_agency)\n",
    "        else:\n",
    "            continue\n",
    "    return global_list_duplicated_sorted    \n",
    "\n",
    "\n",
    "defineSplitSide(global_list_duplicated_sorted)\n",
    "\n",
    "masktest = global_list_duplicated_sorted['name'] == 'Arnaud SIBILLE' \n",
    "global_list_duplicated_sorted.loc[masktest]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_list_duplicated_sorted.to_csv('people_global_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELENIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "#import packages dynamic scraping\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamic website\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    service = Service()\n",
    "    options = webdriver.ChromeOptions\n",
    "    driver = webdriver.Chrome(service=service, options=option)\n",
    "except Exception as ex:\n",
    "    print(f\"An unexpected exception occurred: {ex}\")\n",
    "    # Handle other exceptions\n",
    "\n",
    "finally:\n",
    "    if 'driver' in locals() and driver is not None:\n",
    "        driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
